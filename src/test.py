from langchain_core.messages import HumanMessage, BaseMessage, SystemMessage, AIMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from pydantic import BaseModel
from typing import Annotated, Literal
import functools
from functools import partial
import operator
from typing import Sequence, Optional
from typing_extensions import TypedDict
# from langgraph.prebuilt import create_react_agent
from langgraph.prebuilt import ToolNode
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph, START
from langgraph.checkpoint.memory import MemorySaver
from dotenv import load_dotenv
from langchain_core.tools import tool
from PIL import Image as PILImage
from langchain_core.runnables.graph import MermaidDrawMethod
from io import BytesIO
from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages

load_dotenv()
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)

class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
    next: Optional[str]

# ---------------------------------------------------------------------------
#  Utilities
# ---------------------------------------------------------------------------
def needs_tool(state: AgentState) -> bool:
    """True if the last assistant message contains tool_calls."""
    last = state["messages"][-1]
    return isinstance(last, AIMessage) and bool(getattr(last, "tool_calls", []))

def agent_node(state: AgentState, agent_fn, name: str):
    result = agent_fn(state)
    # forward whatever messages the agent produced
    return {"messages": result["messages"], "next": result["next"]}

# ---------------------------------------------------------------------------
#  Tools
# ---------------------------------------------------------------------------
@tool
def weather_tool() -> str:
    """Simulated weather report."""
    print("Came to weather tool...")
    return "Cloudy with a chance of rain. 16°C."

@tool
def coder_tool() -> str:
    """Simulated coding work."""
    print("Came to coder tool …")
    return "print('Hello, world!')  # generated by coder_tool"

WEATHER_TOOLS = [weather_tool]
CODER_TOOLS = [coder_tool]

# ---------------------------------------------------------------------------
#  Supervisor
# ---------------------------------------------------------------------------
members = ["Weather", "Coder"]
options  = ["FINISH"] + members

class RouteResponse(BaseModel):
    next: Literal["FINISH", "Weather", "Coder"]

system_prompt = """
You are a supervisor managing the following agents: {members}.
Routing rule:
  - Coding questions → Coder
  - Weather related → Weather
  - If no agent is suitable, route to FINISH.
When an agent sets {{"next": "FINISH"}}, reply FINISH.
""".strip()

sup_prompt = (
    ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder("messages"),
            ("system", "Who acts next (or FINISH)? Choose from {options}."),
        ]
    )
    .partial(options=str(options), members=", ".join(members))
)

def supervisor(state: AgentState, llm: ChatOpenAI):
    if state.get("next") == "FINISH":
        return {"next": "FINISH"}
    return (sup_prompt | llm.with_structured_output(RouteResponse)).invoke(state)


# ---------------------------------------------------------------------------
#  Agents
# ---------------------------------------------------------------------------
def weather_agent(state: AgentState, llm: ChatOpenAI):
    last = state["messages"][-1]

    # 2nd pass (tool result already present) -----------------------------
    if isinstance(last, ToolMessage):
        sys = SystemMessage(content="Using the weather_tool output, answer the user.")
        final = llm.invoke([sys] + state["messages"])
        return {"messages": [final], "next": "FINISH"}

    # 1st pass (no tool result yet) --------------------------------------
    sys = SystemMessage(content="Call weather_tool to gather information, then stop.")
    first = llm.bind_tools([weather_tool]).invoke([sys] + state["messages"])
    return {"messages": [first], "next": None}

def coder_agent(state: AgentState, llm: ChatOpenAI):
    last = state["messages"][-1]

    if isinstance(last, ToolMessage):  # 2nd pass
        sys = SystemMessage(content="Using coder_tool output, answer the user.")
        final = llm.invoke([sys] + state["messages"])
        return {"messages": [final], "next": "FINISH"}

    # 1st pass
    sys = SystemMessage(content="Call coder_tool to generate code, then stop.")
    first = llm.bind_tools([coder_tool]).invoke([sys] + state["messages"])
    return {"messages": [first], "next": None}

# ---------------------------------------------------------------------------
#  Build graph
# ---------------------------------------------------------------------------
wf = StateGraph(AgentState)

# Nodes
wf.add_node(
    "Weather",
    functools.partial(agent_node, agent_fn=functools.partial(weather_agent, llm=llm), name="Weather"),
)
wf.add_node(
    "Coder",
    functools.partial(agent_node, agent_fn=functools.partial(coder_agent, llm=llm), name="Coder"),
)
wf.add_node("WeatherTools", ToolNode([weather_tool]))
wf.add_node("CoderTools",      ToolNode([coder_tool]))
wf.add_node("Supervisor", functools.partial(supervisor, llm=llm))

# Entry
wf.add_edge(START, "Supervisor")

# Weather loop
wf.add_conditional_edges(
    "Weather",
    lambda s: "TOOLS" if needs_tool(s) else "SUP",
    {"TOOLS": "WeatherTools", "SUP": "Supervisor"},
)
wf.add_edge("WeatherTools", "Weather")

# Coder loop
wf.add_conditional_edges(
    "Coder",
    lambda s: "TOOLS" if needs_tool(s) else "SUP",
    {"TOOLS": "CoderTools", "SUP": "Supervisor"},
)
wf.add_edge("CoderTools", "Coder")

# Supervisor routing / exit
wf.add_conditional_edges(
    "Supervisor",
    lambda s: s["next"],
    {"Weather": "Weather", "Coder": "Coder", "FINISH": END},
)

graph = wf.compile()

# ---------------------------------------------------------------------
#  Test run
# ---------------------------------------------------------------------

result = graph.invoke(
    {"messages": [
        HumanMessage(content="write hello world in python."),
    ]}
)

print("\nFinal state:\n", result['messages'][-1].content)

result = graph.invoke(
    {"messages": [
        HumanMessage(content="how is today's weather?"),
    ]}
)

print("\nFinal state:\n", result['messages'][-1].content)


# ---------------------------------------------------------------------
#  Visualise graph
# ---------------------------------------------------------------------

# png_bytes = graph.get_graph(xray=True).draw_mermaid_png(
#     draw_method=MermaidDrawMethod.PYPPETEER
# )
# PILImage.open(BytesIO(png_bytes)).show()