from langchain_core.messages import HumanMessage, BaseMessage, SystemMessage, AIMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from pydantic import BaseModel
from typing import Annotated, Literal
import functools
from functools import partial
import operator
from typing import Sequence, Optional
from typing_extensions import TypedDict
# from langgraph.prebuilt import create_react_agent
from langgraph.prebuilt import ToolNode
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph, START
from langgraph.checkpoint.memory import MemorySaver
from dotenv import load_dotenv
from langchain_core.tools import tool
from PIL import Image as PILImage
from langchain_core.runnables.graph import MermaidDrawMethod
from io import BytesIO
from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages

load_dotenv()
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)

class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
    next: Optional[str]

# ---------------------------------------------------------------------------
#  Utilities
# ---------------------------------------------------------------------------
def needs_tool(state: AgentState) -> bool:
    """True if the last assistant message contains tool_calls."""
    last = state["messages"][-1]
    return isinstance(last, AIMessage) and bool(getattr(last, "tool_calls", []))

def agent_node(state: AgentState, agent_fn, name: str):
    result = agent_fn(state)
    # forward whatever messages the agent produced
    return {"messages": result["messages"], "next": result["next"]}

# ---------------------------------------------------------------------------
#  Tools
# ---------------------------------------------------------------------------
@tool
def research_tool() -> str:
    """Simulated research work."""
    print("Came to research tool …")
    return "Hi, I am a research tool that can search the web for information."

@tool
def coder_tool() -> str:
    """Simulated coding work."""
    print("Came to coder tool …")
    return "print('Hello, world!')  # generated by coder_tool"

# TOOLS = [research_tool, coder_tool]
RESEARCHER_TOOLS = [research_tool]
CODER_TOOLS = [coder_tool]

# ---------------------------------------------------------------------------
#  Supervisor
# ---------------------------------------------------------------------------
members = ["Researcher", "Coder"]
options  = ["FINISH"] + members

class RouteResponse(BaseModel):
    next: Literal["FINISH", "Researcher", "Coder"]

system_prompt = """
You are a supervisor managing the following agents: {members}.
Routing rule:
  - Coding questions → Coder
  - Everything else → Researcher
When an agent sets {{"next": "FINISH"}}, reply FINISH.
""".strip()

sup_prompt = (
    ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder("messages"),
            ("system", "Who acts next (or FINISH)? Choose from {options}."),
        ]
    )
    .partial(options=str(options), members=", ".join(members))
)

def supervisor(state: AgentState, llm: ChatOpenAI):
    if state.get("next") == "FINISH":
        return {"next": "FINISH"}
    return (sup_prompt | llm.with_structured_output(RouteResponse)).invoke(state)


# ---------------------------------------------------------------------------
#  Agents
# ---------------------------------------------------------------------------
def research_agent(state: AgentState, llm: ChatOpenAI):
    last = state["messages"][-1]

    # 2nd pass (tool result already present) -----------------------------
    if isinstance(last, ToolMessage):
        sys = SystemMessage(content="Using the research_tool output, answer the user.")
        final = llm.invoke([sys] + state["messages"])
        return {"messages": [final], "next": "FINISH"}

    # 1st pass (no tool result yet) --------------------------------------
    sys = SystemMessage(content="Call research_tool to gather information, then stop.")
    first = llm.bind_tools([research_tool]).invoke([sys] + state["messages"])
    return {"messages": [first], "next": None}

def coder_agent(state: AgentState, llm: ChatOpenAI):
    last = state["messages"][-1]

    if isinstance(last, ToolMessage):  # 2nd pass
        sys = SystemMessage(content="Using coder_tool output, answer the user.")
        final = llm.invoke([sys] + state["messages"])
        return {"messages": [final], "next": "FINISH"}

    # 1st pass
    sys = SystemMessage(content="Call coder_tool to generate code, then stop.")
    first = llm.bind_tools([coder_tool]).invoke([sys] + state["messages"])
    return {"messages": [first], "next": None}

# ---------------------------------------------------------------------------
#  Build graph
# ---------------------------------------------------------------------------
wf = StateGraph(AgentState)

# Nodes
wf.add_node(
    "Researcher",
    functools.partial(agent_node, agent_fn=functools.partial(research_agent, llm=llm), name="Researcher"),
)
wf.add_node(
    "Coder",
    functools.partial(agent_node, agent_fn=functools.partial(coder_agent, llm=llm), name="Coder"),
)
wf.add_node("ResearcherTools", ToolNode([research_tool]))
wf.add_node("CoderTools",      ToolNode([coder_tool]))
wf.add_node("Supervisor", functools.partial(supervisor, llm=llm))

# Entry
wf.add_edge(START, "Supervisor")

# Researcher loop
wf.add_conditional_edges(
    "Researcher",
    lambda s: "TOOLS" if needs_tool(s) else "SUP",
    {"TOOLS": "ResearcherTools", "SUP": "Supervisor"},
)
wf.add_edge("ResearcherTools", "Researcher")

# Coder loop
wf.add_conditional_edges(
    "Coder",
    lambda s: "TOOLS" if needs_tool(s) else "SUP",
    {"TOOLS": "CoderTools", "SUP": "Supervisor"},
)
wf.add_edge("CoderTools", "Coder")

# Supervisor routing / exit
wf.add_conditional_edges(
    "Supervisor",
    lambda s: s["next"],
    {"Researcher": "Researcher", "Coder": "Coder", "FINISH": END},
)

graph = wf.compile()

# ---------------------------------------------------------------------
#  Test run
# ---------------------------------------------------------------------

# result = graph.invoke(
#     {"messages": [
#         HumanMessage(content="write hello world in python."),
#     ]}
# )

# print("\nFinal state:\n", result['messages'][-1].content)

# result = graph.invoke(
#     {"messages": [
#         HumanMessage(content="what does a reasearcher do?"),
#     ]}
# )

# print("\nFinal state:\n", result['messages'][-1].content)


# ---------------------------------------------------------------------
#  Visualise graph
# ---------------------------------------------------------------------

png_bytes = graph.get_graph(xray=True).draw_mermaid_png(
    draw_method=MermaidDrawMethod.PYPPETEER
)
PILImage.open(BytesIO(png_bytes)).show()